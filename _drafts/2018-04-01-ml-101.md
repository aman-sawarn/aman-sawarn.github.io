---
layout: post
title:  "Machine Learning with Words: A Python Primer"
date:   2018-05-01
---

# Machine Learning with Words: A Python Primer

If you're curious about machine learning and want to get started coding as quickly as possible, this primer is for you! My goal is to get you running a machine learning pipeline on your own dataset.

You don’t need any background in machine learning to follow this primer, but I’ll assume that you know enough programming to follow my Python snippets.

<br><br>

## Requirements

If you're new to Python, download [Anaconda](https://www.continuum.io/anaconda-overview). It has everything you need! I recommend getting the version for Python 3.6, which is the version I use in this tutorial.

Optional: You might want to install [PyCharm](https://www.jetbrains.com/pycharm/) to make writing and debugging your code a bit easier. You can also use a [Jupyter notebook](http://jupyter.org/) to write and run your code interactively. Jupyter is included in Anaconda and can be run within PyCharm.

If you already have Python 3.6, you'll just need following packages:

* [pandas](http://pandas.pydata.org/)
* [scikit-learn](http://scikit-learn.org/)
* [gensim](https://radimrehurek.com/gensim/)
* [spaCy](https://spacy.io/)
* [matplotlib](https://matplotlib.org/)
* [seaborn](https://seaborn.pydata.org/)

<br><br>

## Find a dataset

You can use any text dataset you like. I’ll be using a collection of Shakespeare plays from [Folger Digital Texts](http://www.folgerdigitaltexts.org/).

To make your first project easier, you might want to choose a dataset with the following qualities:

* machine-readable text (.txt or .csv files, not PDF)
* 1,000-100,000 documents
* a topic that is familiar to you

<br><br>

## Load and clean your dataset

A **document** is a unit within your dataset. A document might be one sentence, one paragraph, one chapter, one tweet, one book... The size is up to you!

Use **pandas** to load your data and divide it into documents.

{% highlight python %}
import pandas as pd

pd.load_csv('/path/to/your/documents')
{% endhighlight %}

Clean the documents by removing numbers and punctuation and lowercasing the text. You should also remove duplicate documents from your dataset.

{% highlight python %}
def remove_non_alpha(text):
    return re.sub('[^A-Za-z\s]', ' ', text)

def preprocess_text(text, stopwords=None):
    text = text.lower()
    text = remove_non_alpha(text)
    return text
{% endhighlight %}

<br><br>

## Create feature vectors.

A [feature vector](https://en.wikipedia.org/wiki/Feature_vector) is a machine readable representation of a document. It includes a list of **features** and **values**. The values can indicate whether a feature is present or how important it is. For text documents, feature vectors usually consist of words (features) and their frequencies (values). The intuition is that if a word occurs more often in a document, it usually means that the word is important.

For example, the sentence *A fool thinks himself to be wise, but a wise man knows himself to be a fool.* can be transformed into the following feature vector:

| a  | be  | but  | fool  | himself | knows | man  | thinks  | to  | wise  |   
|----|-----|------|-------|---------|-------|------|---------|-----|-------|
| 3  | 2   | 1    | 2     | 2       | 1     | 1    | 1       | 2   | 2     |

Unfortunately, using just the words and frequencies can result in some errors. Here are a few problems to consider:

* Documents have different lengths, so the frequency of a word within one document might not be comparable to its frequency in another document. For example, if a word occurs 10 times in a document of length 20, it is probably more important than a word that occurs 10 times in a document of length 200.

* Some words occur very frequently and are distributed (somewhat) evenly across all documents (e.g. *the*, *and*, *she*). We sometimes call these [stopwords](https://en.wikipedia.org/wiki/Stop_words). Although they have high frequencies, they are usually not significant since all documents have these words.

* Many, many words occur only a few times in the entire dataset. This is known as the [long tail](https://en.wikipedia.org/wiki/Long_tail). These words simply don't occur frequently enough to tell us anything useful, and they also explode our feature space (the more features you have, the slower your algorithm will run).

Luckily, we can solve many of these problems by using some simple hacks (remove stopwords and rare words) and weighting our feature values using [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf). TF-IDF compares the **term frequency** (the frequency of a word in the current document) to the **document frequency** (the distribution of the word across all the documents in the dataset).

This can all be done very easily if we use **scikit-learn** to create our feature vectors.

{% highlight python %}
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(norm='l2',
                             use_idf=True,
                             min_df=10,
                             sublinear_tf=True)

vectorizer = vectorizer.fit(data_df['processed text'].tolist())
vectors = vectorizer.transform(data_df['processed text'].tolist())
{% endhighlight %}

Let's break that down a bit:
* By specifying `min_df=10` we've required that each word occur in at least 10 documents. This eliminates rare words from our features.
* By specifying `use_idf=True`, we say that yes, we want to use the document frequency to weaken the effect of frequent words that occur in most of the documents and strengthen the effect of words that occur less frequently but in specific documents.
* By specifying `sublinear_tf=True`, we scale the feature values so that very frequent features don't overwhelm the vectors.
* By specifying, `norm=l2`, we normalize the vectors to remove the bias introduced by different document lengths.

Further Reading:
* <https://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html>
* <http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html>
* <http://blog.christianperone.com/2011/10/machine-learning-text-feature-extraction-tf-idf-part-ii/>

<br><br>

## Unsupervised Learning

[Unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning) is when we give the computer only the dataset, without any other information, and ask it to find some hidden structure. Since the computer doesn't know exactly what you're looking for, it might not generate what you expect, but the results can still be useful.

For example, you might ask the computer to [cluster](https://en.wikipedia.org/wiki/Cluster_analysis) your dataset (divide the dataset into groups containing similar documents). If you had a dataset of movie scripts, you might have expected the computer to divide the scripts by genre, but it might latch onto some other hidden structure, such as language.

Unsupervised techniques are especially good for exploring a new dataset. You might discover that part of your dataset

<br><br>

## Cluster your dataset

Let's begin by asking the computer to **cluster** your dataset. Clustering means gathering the documents into groups based on their similarity.

How it works: K-means searches for

<br><br>

## Discover topics in your dataset

<br><br>

## Supervised Learning

[Supervised learning](https://en.wikipedia.org/wiki/Supervised_learning) is when we give the computer both a dataset and labels.

<br><br>

## Label and classify your dataset

<br><br>

### Preparing data

First we need to separate our dataset into a **training set** and a **test set**.

Then we need to get vectors and **class labels** for each document in the training and test sets. We can use the same TF-IDF vectorizer as before, and the class labels should correspond to the categories that you want to identify.

For example, I might want to build a classifier for my Shakespeare dataset that predicts the name of the play for each document. My classes are the names of the plays.

<br><br>

### Training

<br><br>

### Evaluation

First, let's get the predicted labels for our test set.

{% highlight python %}
predicted_labels = model.predict(test_vectors)
{% endhighlight %}

Now we need to check how accurate the labels are. There are many ways to evaluate your classifier, the three standard scores are:
* [Precision](https://en.wikipedia.org/wiki/Precision_and_recall): Of the documents we labeled X, how many of those were actually labeled X?
* [Recall](https://en.wikipedia.org/wiki/Precision_and_recall): Of the documents with the label X, how many of those did we classify as X?
* [F1](https://en.wikipedia.org/wiki/F1_score): A combination of precision and recall; you can think of it as a summary statistic.

All of these scores are between 0 and 1, and the higher they are, the better.

To quickly get precision, recall, and F1 for each class, we can ask for a classification report:

{% highlight python %}
from sklearn.metrics import classification_report
print(classification_report(test_labels, predicted_labels))
{% endhighlight %}

We can also examine the feature importances. These tell us which of our features most influence the classifier.

As one may have expected, we see that my classifier has been "cheating" by using the character names to make the classification

<br><br>

## Resources to learn more


<br><br><br>
